Virtual Machine Information:

Virtual Machine
Name
IP
Floating IP

Master
g11-project-master
192.168.2.197
130.238.28.151

Worker1
g11-project-worker1
192.168.2.107
130.238.29.111

Worker2 (hasn’t connected to master yet)
g11-project-worker2
192.168.2.86
130.238.29.102

Worker3 (hasn’t connected to master yet)
g11-project-worker3
192.168.2.27
130.238.29.123




Your Local Laptop settings:

Key pair’s name is project.pem, which has been sent to the whatsapp group.
You can put project.pem under ~/.ssh

Give the .pem file authority.
chmod 400 project.pem

Configure the ~/.ssh/config on your local laptop/desktop/lab computer to this
# This will open ssh-tunnels (for the given ports) between your local computer and the VM. 

Host 130.238.28.151
  User ubuntu
  # modify this to match the name of your key
  IdentityFile ~/.ssh/project.pem
  # Spark master web GUI
  LocalForward 8080 192.168.2.197:8080
  # HDFS namenode web gui
  LocalForward 9870 192.168.2.197:9870
  # python notebook
  LocalForward 8888 localhost:8888
  # spark applications
  LocalForward 4040 localhost:4040
  LocalForward 4041 localhost:4041
  LocalForward 4042 localhost:4042
  LocalForward 4043 localhost:4043
  LocalForward 4044 localhost:4044
  LocalForward 4045 localhost:4045
  LocalForward 4046 localhost:4046
  LocalForward 4047 localhost:4047
  LocalForward 4048 localhost:4048
  LocalForward 4049 localhost:4049
  LocalForward 4050 localhost:4050
  LocalForward 4051 localhost:4051
  LocalForward 4052 localhost:4052
  LocalForward 4053 localhost:4053
  LocalForward 4054 localhost:4054
  LocalForward 4055 localhost:4055
  LocalForward 4056 localhost:4056
  LocalForward 4057 localhost:4057
  LocalForward 4058 localhost:4058
  LocalForward 4059 localhost:4059
  LocalForward 4060 localhost:4060


# This will enable to access to our cluster
ssh 130.238.28.151



Application Directory

For both master and workers:

Hadoop:
Hadoop package:
/home/ubuntu/hadoop-3.3.0/

Loading edits:
/home/ubuntu/hadoop-3.3.0/data/nameNode/current/


Spark:
Spark package:
/home/ubuntu/spark-3.2.1-bin-hadoop3.2



Some Useful Commands

# Enter one worker, for example:
ssh g11-project-worker1

# Exit one worker
exit

# Start cluster
start-all.sh

# End cluster
stop-all.sh

# Check daemons
jps

# start the notebook
python3 -m jupyterlab

# See all data files in hdfs
hdfs dfs -ls



Web Address

HDFS:
http://130.238.28.151:9870/


Spark:
http://130.238.28.151:8080/
